# Imbalanced Learning Techniques

Imbalanced learning is a common challenge in machine learning, where the number of samples in one class is significantly higher or lower than the number of samples in the other classes. This can result in biased models that perform poorly on the minority class. To address this problem, various techniques have been developed, such as: oversampling, undersampling, and synthetic data generation.

In this assignment, we will focus on imbalanced learning and how it can be addressed using oversampling or undersampling techniques. We will explore the impact of these techniques on the performance of several machine learning models, including decision trees, logistic regression, and neural networks. Specifically, we will compare the performance of these models on an imbalanced dataset without any modifications, versus the same models trained on datasets that have been oversampled or undersampled to balance the class distribution.
<br>
<br>
<b>The techniques that will be utilized are :</b>
<li>Synthetic data generation oversampling</li>
<li>Cluster based undersampling</li>
<li>Easy Ensemble</li>
<br>
<b>The following models are :</b>
<li>Random Forest</li>
<li>Support Vector Machine (SVM)</li>
<li>Naive Bayes</li>
<br>
<b>The Goal</b>
The goal of this assignment is to understand the trade-offs of using oversampling or undersampling in machine learning, and to evaluate the effectiveness of different models in dealing with imbalanced datasets.
